{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import window as W\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession \n",
    "    .builder \n",
    "    .appName(\"Streaming from Kafka\") \n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0')\n",
    "    .config(\"spark.driver.extraClassPath\", \"./jdbc/mysql-connector-j-8.4.0.jar\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 8)\n",
    "    .master(\"local[*]\") \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:19091,kafka2:19092,kafka3:19093\") \\\n",
    "    .option(\"subscribe\", \"user\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField(\"birthdate\", T.StringType()),\n",
    "    T.StructField(\"blood_group\", T.StringType()),\n",
    "    T.StructField(\"job\", T.StringType()),\n",
    "    T.StructField(\"name\", T.StringType()),\n",
    "    T.StructField(\"residence\", T.StringType()),\n",
    "    T.StructField(\"sex\", T.StringType()),\n",
    "    T.StructField(\"ssn\", T.StringType()),\n",
    "    T.StructField(\"uuid\", T.StringType()),\n",
    "    T.StructField(\"timestamp\", T.TimestampType()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "value_df = kafka_df.select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "\n",
    "processed_df = value_df.selectExpr(\n",
    "    \"value.birthdate\", \n",
    "    \"value.blood_group\", \n",
    "    \"value.job\",\n",
    "    \"value.name\",\n",
    "    \"value.residence\",\n",
    "    \"value.sex\",\n",
    "    \"value.ssn\",\n",
    "    \"value.uuid\",\n",
    "    \"value.timestamp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_agg = processed_df \\\n",
    "    .withWatermark(\"timestamp\", \"5 seconds\") \\\n",
    "    .groupBy(F.window(\"timestamp\", \"5 seconds\"),\n",
    "                          \"uuid\").count()\n",
    "\n",
    "# df_order = df_agg.groupby(\"window\").count().orderBy('window')\n",
    "\n",
    "df_final = df_agg.selectExpr(\"window.start as start_time\", \"window.end as end_time\", 'uuid', \"count\").orderBy('start_time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Multiple streaming aggregations are not supported with streaming DataFrames/Datasets;\nProject [window#41-T5000ms.start AS start_time#64, window#41-T5000ms.end AS end_time#65, count#61L]\n+- Sort [window#41-T5000ms ASC NULLS FIRST], true\n   +- Aggregate [window#41-T5000ms], [window#41-T5000ms, count(1) AS count#61L]\n      +- Aggregate [window#53-T5000ms, uuid#30], [window#53-T5000ms AS window#41-T5000ms, uuid#30, count(1) AS count#52L]\n         +- Project [named_struct(start, precisetimestampconversion(((precisetimestampconversion(timestamp#31-T5000ms, TimestampType, LongType) - (((precisetimestampconversion(timestamp#31-T5000ms, TimestampType, LongType) - 0) + 5000000) % 5000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(timestamp#31-T5000ms, TimestampType, LongType) - (((precisetimestampconversion(timestamp#31-T5000ms, TimestampType, LongType) - 0) + 5000000) % 5000000)) - 0) + 5000000), LongType, TimestampType)) AS window#53-T5000ms, birthdate#23, blood_group#24, job#25, name#26, residence#27, sex#28, ssn#29, uuid#30, timestamp#31-T5000ms]\n            +- Filter isnotnull(timestamp#31-T5000ms)\n               +- EventTimeWatermark timestamp#31: timestamp, 5 seconds\n                  +- Project [value#21.birthdate AS birthdate#23, value#21.blood_group AS blood_group#24, value#21.job AS job#25, value#21.name AS name#26, value#21.residence AS residence#27, value#21.sex AS sex#28, value#21.ssn AS ssn#29, value#21.uuid AS uuid#30, value#21.timestamp AS timestamp#31]\n                     +- Project [from_json(StructField(birthdate,StringType,true), StructField(blood_group,StringType,true), StructField(job,StringType,true), StructField(name,StringType,true), StructField(residence,StringType,true), StructField(sex,StringType,true), StructField(ssn,StringType,true), StructField(uuid,StringType,true), StructField(timestamp,TimestampType,true), cast(value#8 as string), Some(Etc/UTC)) AS value#21]\n                        +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@51b1f651, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@76042dd7, [startingOffsets=latest, kafka.bootstrap.servers=kafka1:19091,kafka2:19092,kafka3:19093, subscribe=user], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3db85c46,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:19091,kafka2:19092,kafka3:19093, subscribe -> user, startingOffsets -> latest),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17836/1512293352.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpointLocation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoint_dir/05_window_operations_and_watermarks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"5 seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Multiple streaming aggregations are not supported with streaming DataFrames/Datasets;\nProject [window#41-T5000ms.start AS start_time#64, window#41-T5000ms.end AS end_time#65, count#61L]\n+- Sort [window#41-T5000ms ASC NULLS FIRST], true\n   +- Aggregate [window#41-T5000ms], [window#41-T5000ms, count(1) AS count#61L]\n      +- Aggregate [window#53-T5000ms, uuid#30], [window#53-T5000ms AS window#41-T5000ms, uuid#30, count(1) AS count#52L]\n         +- Project [named_struct(start, precisetimestampconversion(((precisetimestampconversion(timestamp#31-T5000ms, TimestampType, LongType) - (((precisetimestampconversion(timestamp#31-T5000ms, TimestampType, LongType) - 0) + 5000000) % 5000000)) - 0), LongType, TimestampType), end, precisetimestampconversion((((precisetimestampconversion(timestamp#31-T5000ms, TimestampType, LongType) - (((precisetimestampconversion(timestamp#31-T5000ms, TimestampType, LongType) - 0) + 5000000) % 5000000)) - 0) + 5000000), LongType, TimestampType)) AS window#53-T5000ms, birthdate#23, blood_group#24, job#25, name#26, residence#27, sex#28, ssn#29, uuid#30, timestamp#31-T5000ms]\n            +- Filter isnotnull(timestamp#31-T5000ms)\n               +- EventTimeWatermark timestamp#31: timestamp, 5 seconds\n                  +- Project [value#21.birthdate AS birthdate#23, value#21.blood_group AS blood_group#24, value#21.job AS job#25, value#21.name AS name#26, value#21.residence AS residence#27, value#21.sex AS sex#28, value#21.ssn AS ssn#29, value#21.uuid AS uuid#30, value#21.timestamp AS timestamp#31]\n                     +- Project [from_json(StructField(birthdate,StringType,true), StructField(blood_group,StringType,true), StructField(job,StringType,true), StructField(name,StringType,true), StructField(residence,StringType,true), StructField(sex,StringType,true), StructField(ssn,StringType,true), StructField(uuid,StringType,true), StructField(timestamp,TimestampType,true), cast(value#8 as string), Some(Etc/UTC)) AS value#21]\n                        +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@51b1f651, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@76042dd7, [startingOffsets=latest, kafka.bootstrap.servers=kafka1:19091,kafka2:19092,kafka3:19093, subscribe=user], [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@3db85c46,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> kafka1:19091,kafka2:19092,kafka3:19093, subscribe -> user, startingOffsets -> latest),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n"
     ]
    }
   ],
   "source": [
    "df_console = df_final.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint_dir/05_window_operations_and_watermarks\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "df_console.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/local/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14981/3347510699.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# df_memory = processed_df.writeStream \\\n",
    "#     .format(\"memory\") \\\n",
    "#     .queryName(\"kafka_memory\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .option(\"checkpointLocation\", \"checkpoint_dir/05_window_operations_and_watermarks\") \\\n",
    "#     .trigger(processingTime=\"5 seconds\") \\\n",
    "#     .start()\n",
    "\n",
    "# df_memory.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------+-----------------+--------------------+---+-----------+--------------------+-------------------+\n",
      "|birthdate|blood_group|                 job|             name|           residence|sex|        ssn|                uuid|          timestamp|\n",
      "+---------+-----------+--------------------+-----------------+--------------------+---+-----------+--------------------+-------------------+\n",
      "| 19670613|         O+|  Analytical chemist| Michael Erickson|80800 Pamela Cany...|  M|254-75-7053|LJoSb6NWRWW27Zkpn...|2024-05-12 15:32:33|\n",
      "| 20211103|         A-|Estate manager/la...|     Andrea Lopez|440 Wise Burgs\\nP...|  F|822-34-7088|f9i6J3xv2FojTszsh...|2024-05-12 15:32:34|\n",
      "| 20040509|         B+|Human resources o...|      Lisa Tanner|USNS Walker\\nFPO ...|  F|764-39-7261|jnVioyWFLrei7qRMF...|2024-05-12 15:32:35|\n",
      "| 19980121|         O+|        Retail buyer|     Karen Bryant|929 Natalie Mount...|  F|192-41-8135|SEcJmNfmbh3bjixAs...|2024-05-12 15:32:36|\n",
      "| 19290418|        AB-|Research officer,...|    Sarah Carlson|USNV Rodriguez\\nF...|  F|150-78-6981|ZjaTjVETGfA9KUmHk...|2024-05-12 15:32:37|\n",
      "| 19150819|         O+|Clinical research...|     Kyle Kennedy|PSC 9330, Box 837...|  M|049-07-5244|3tjTJMzvNK2inXjH2...|2024-05-12 15:32:38|\n",
      "| 19861122|        AB-|               Actor|    Danielle Mack|588 Brooks Prairi...|  F|028-17-2782|drHoMCsYPHbZkGFez...|2024-05-12 15:32:39|\n",
      "| 19230307|         A-|Emergency plannin...|  Jeremy Morrison|379 Heather Summi...|  M|172-02-2883|eHHXURBAujxKjiVyd...|2024-05-12 15:32:40|\n",
      "| 19301019|         A-|Data processing m...|Elizabeth Manning|66170 Brianna Squ...|  F|702-90-4833|E5GxZkgKm6Y6LVLBL...|2024-05-12 15:32:41|\n",
      "| 19601011|         O+|Psychologist, spo...|     Joseph Eaton|13005 Johnson Gre...|  M|806-02-5415|5HuyHRHwMmL7EQSdE...|2024-05-12 15:32:43|\n",
      "| 19090725|         B+|         Media buyer|    Kelsey Wilson|Unit 2448 Box 556...|  F|792-52-9550|TwXgbE5JKeDQKkXWE...|2024-05-12 15:32:44|\n",
      "| 20071015|         A-|    Location manager|      Laura Baker|06107 Jeffrey Rap...|  F|361-38-2808|GKFPNaPbfDconxg8X...|2024-05-12 15:32:45|\n",
      "| 20130630|        AB-|       Herpetologist|     Donald Frank|8445 Singleton Hi...|  M|291-61-5619|cpzRUHntSCDLTehYN...|2024-05-12 15:32:46|\n",
      "| 19090205|         A+|Government social...|     Adam Stevens|9315 Whitehead Kn...|  M|010-45-6713|XQk69F5Rik4yjW6Uq...|2024-05-12 15:32:47|\n",
      "| 19850623|        AB+|Architectural tec...|    Sarah Collins|76660 Monica Summ...|  F|507-98-0049|GAkfdn3Q3zpAtgaJ2...|2024-05-12 15:32:48|\n",
      "| 19541202|         B-|Magazine features...|    Sandra Mendez|7327 Boyle Traffi...|  F|427-77-8175|mNneqnBgnnneD23M4...|2024-05-12 15:32:49|\n",
      "+---------+-----------+--------------------+-----------------+--------------------+---+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM kafka_memory\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_agg = df \\\n",
    "    .withWatermark(\"timestamp\", \"5 seconds\") \\\n",
    "    .groupBy(F.window(\"timestamp\", \"5 seconds\"),\n",
    "                          \"uuid\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----------------------+-----+\n",
      "|window                                    |uuid                  |count|\n",
      "+------------------------------------------+----------------------+-----+\n",
      "|{2024-05-12 15:32:35, 2024-05-12 15:32:40}|jnVioyWFLrei7qRMFLNs3r|1    |\n",
      "|{2024-05-12 15:32:30, 2024-05-12 15:32:35}|f9i6J3xv2FojTszshGHQx5|1    |\n",
      "|{2024-05-12 15:32:35, 2024-05-12 15:32:40}|SEcJmNfmbh3bjixAscKdpp|1    |\n",
      "|{2024-05-12 15:32:30, 2024-05-12 15:32:35}|LJoSb6NWRWW27ZkpnFYjvq|1    |\n",
      "|{2024-05-12 15:32:40, 2024-05-12 15:32:45}|eHHXURBAujxKjiVydguqsm|1    |\n",
      "|{2024-05-12 15:32:35, 2024-05-12 15:32:40}|ZjaTjVETGfA9KUmHkHweEf|1    |\n",
      "|{2024-05-12 15:32:35, 2024-05-12 15:32:40}|3tjTJMzvNK2inXjH2ii84Y|1    |\n",
      "|{2024-05-12 15:32:35, 2024-05-12 15:32:40}|drHoMCsYPHbZkGFezqStS5|1    |\n",
      "|{2024-05-12 15:32:40, 2024-05-12 15:32:45}|5HuyHRHwMmL7EQSdEnPSBk|1    |\n",
      "|{2024-05-12 15:32:40, 2024-05-12 15:32:45}|E5GxZkgKm6Y6LVLBLm3Y8n|1    |\n",
      "|{2024-05-12 15:32:40, 2024-05-12 15:32:45}|TwXgbE5JKeDQKkXWEdmHid|1    |\n",
      "|{2024-05-12 15:32:45, 2024-05-12 15:32:50}|GKFPNaPbfDconxg8XjBxr5|1    |\n",
      "|{2024-05-12 15:32:45, 2024-05-12 15:32:50}|GAkfdn3Q3zpAtgaJ2aztgX|1    |\n",
      "|{2024-05-12 15:32:45, 2024-05-12 15:32:50}|XQk69F5Rik4yjW6UqPKTpc|1    |\n",
      "|{2024-05-12 15:32:45, 2024-05-12 15:32:50}|mNneqnBgnnneD23M4Q8J3Z|1    |\n",
      "|{2024-05-12 15:32:45, 2024-05-12 15:32:50}|cpzRUHntSCDLTehYNbuiEi|1    |\n",
      "+------------------------------------------+----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_order = df_agg.groupby(\"window\").count().orderBy('window')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----+\n",
      "|start_time         |end_time           |count|\n",
      "+-------------------+-------------------+-----+\n",
      "|2024-05-12 15:32:30|2024-05-12 15:32:35|2    |\n",
      "|2024-05-12 15:32:35|2024-05-12 15:32:40|5    |\n",
      "|2024-05-12 15:32:40|2024-05-12 15:32:45|4    |\n",
      "|2024-05-12 15:32:45|2024-05-12 15:32:50|5    |\n",
      "+-------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final = df_order.selectExpr(\"window.start as start_time\", \"window.end as end_time\", \"count\")\n",
    "\n",
    "df_final.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
