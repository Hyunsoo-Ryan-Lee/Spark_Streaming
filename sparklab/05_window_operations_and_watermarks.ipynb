{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import window as W\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession \n",
    "    .builder \n",
    "    .appName(\"Streaming from Kafka\") \n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0')\n",
    "    .config(\"spark.driver.extraClassPath\", \"./jdbc/mysql-connector-j-8.4.0.jar\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 8)\n",
    "    .master(\"local[*]\") \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdf = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:19091,kafka2:19092,kafka3:19093\") \\\n",
    "    .option(\"subscribe\", \"demo1\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "kdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n",
      "| key|               value|topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6E 61 6D 6...|demo1|        0|     0|2024-05-13 12:54:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...|demo1|        0|     1|2024-05-13 12:54:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...|demo1|        1|     0|2024-05-13 12:54:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...|demo1|        2|     0|2024-05-13 12:54:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...|demo1|        4|     0|2024-05-13 12:54:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...|demo1|        3|     0|2024-05-13 12:54:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...|demo1|        3|     1|2024-05-13 12:54:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...|demo1|        3|     2|2024-05-13 12:54:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...|demo1|        3|     3|2024-05-13 12:54:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...|demo1|        3|     4|2024-05-13 12:54:...|            0|\n",
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:19091,kafka2:19092,kafka3:19093\") \\\n",
    "    .option(\"subscribe\", \"user\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField(\"birthdate\", T.StringType()),\n",
    "    T.StructField(\"blood_group\", T.StringType()),\n",
    "    T.StructField(\"job\", T.StringType()),\n",
    "    T.StructField(\"name\", T.StringType()),\n",
    "    T.StructField(\"residence\", T.StringType()),\n",
    "    T.StructField(\"sex\", T.StringType()),\n",
    "    T.StructField(\"ssn\", T.StringType()),\n",
    "    T.StructField(\"uuid\", T.StringType()),\n",
    "    T.StructField(\"timestamp\", T.TimestampType()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "value_df = kafka_df.select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "\n",
    "processed_df = value_df.selectExpr(\n",
    "    \"value.birthdate\", \n",
    "    \"value.blood_group\", \n",
    "    \"value.job\",\n",
    "    \"value.name\",\n",
    "    \"value.residence\",\n",
    "    \"value.sex\",\n",
    "    \"value.ssn\",\n",
    "    \"value.uuid\",\n",
    "    \"value.timestamp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_agg = processed_df \\\n",
    "    .withWatermark(\"timestamp\", \"5 seconds\") \\\n",
    "    .groupBy(F.window(\"timestamp\", \"5 seconds\"),\n",
    "                          \"uuid\").count()\n",
    "\n",
    "# df_order = df_agg.groupby(\"window\").count().orderBy('window')\n",
    "\n",
    "df_final = df_agg.selectExpr(\"window.start as start_time\", \"window.end as end_time\", 'uuid', \"count\").orderBy('start_time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_console = df_final.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint_dir/05_window_operations_and_watermarks\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "df_console.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/local/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14981/3347510699.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# df_memory = processed_df.writeStream \\\n",
    "#     .format(\"memory\") \\\n",
    "#     .queryName(\"kafka_memory\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .option(\"checkpointLocation\", \"checkpoint_dir/05_window_operations_and_watermarks\") \\\n",
    "#     .trigger(processingTime=\"5 seconds\") \\\n",
    "#     .start()\n",
    "\n",
    "# df_memory.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------+-----------------+--------------------+---+-----------+--------------------+-------------------+\n",
      "|birthdate|blood_group|                 job|             name|           residence|sex|        ssn|                uuid|          timestamp|\n",
      "+---------+-----------+--------------------+-----------------+--------------------+---+-----------+--------------------+-------------------+\n",
      "| 19670613|         O+|  Analytical chemist| Michael Erickson|80800 Pamela Cany...|  M|254-75-7053|LJoSb6NWRWW27Zkpn...|2024-05-12 15:32:33|\n",
      "| 20211103|         A-|Estate manager/la...|     Andrea Lopez|440 Wise Burgs\\nP...|  F|822-34-7088|f9i6J3xv2FojTszsh...|2024-05-12 15:32:34|\n",
      "| 20040509|         B+|Human resources o...|      Lisa Tanner|USNS Walker\\nFPO ...|  F|764-39-7261|jnVioyWFLrei7qRMF...|2024-05-12 15:32:35|\n",
      "| 19980121|         O+|        Retail buyer|     Karen Bryant|929 Natalie Mount...|  F|192-41-8135|SEcJmNfmbh3bjixAs...|2024-05-12 15:32:36|\n",
      "| 19290418|        AB-|Research officer,...|    Sarah Carlson|USNV Rodriguez\\nF...|  F|150-78-6981|ZjaTjVETGfA9KUmHk...|2024-05-12 15:32:37|\n",
      "| 19150819|         O+|Clinical research...|     Kyle Kennedy|PSC 9330, Box 837...|  M|049-07-5244|3tjTJMzvNK2inXjH2...|2024-05-12 15:32:38|\n",
      "| 19861122|        AB-|               Actor|    Danielle Mack|588 Brooks Prairi...|  F|028-17-2782|drHoMCsYPHbZkGFez...|2024-05-12 15:32:39|\n",
      "| 19230307|         A-|Emergency plannin...|  Jeremy Morrison|379 Heather Summi...|  M|172-02-2883|eHHXURBAujxKjiVyd...|2024-05-12 15:32:40|\n",
      "| 19301019|         A-|Data processing m...|Elizabeth Manning|66170 Brianna Squ...|  F|702-90-4833|E5GxZkgKm6Y6LVLBL...|2024-05-12 15:32:41|\n",
      "| 19601011|         O+|Psychologist, spo...|     Joseph Eaton|13005 Johnson Gre...|  M|806-02-5415|5HuyHRHwMmL7EQSdE...|2024-05-12 15:32:43|\n",
      "| 19090725|         B+|         Media buyer|    Kelsey Wilson|Unit 2448 Box 556...|  F|792-52-9550|TwXgbE5JKeDQKkXWE...|2024-05-12 15:32:44|\n",
      "| 20071015|         A-|    Location manager|      Laura Baker|06107 Jeffrey Rap...|  F|361-38-2808|GKFPNaPbfDconxg8X...|2024-05-12 15:32:45|\n",
      "| 20130630|        AB-|       Herpetologist|     Donald Frank|8445 Singleton Hi...|  M|291-61-5619|cpzRUHntSCDLTehYN...|2024-05-12 15:32:46|\n",
      "| 19090205|         A+|Government social...|     Adam Stevens|9315 Whitehead Kn...|  M|010-45-6713|XQk69F5Rik4yjW6Uq...|2024-05-12 15:32:47|\n",
      "| 19850623|        AB+|Architectural tec...|    Sarah Collins|76660 Monica Summ...|  F|507-98-0049|GAkfdn3Q3zpAtgaJ2...|2024-05-12 15:32:48|\n",
      "| 19541202|         B-|Magazine features...|    Sandra Mendez|7327 Boyle Traffi...|  F|427-77-8175|mNneqnBgnnneD23M4...|2024-05-12 15:32:49|\n",
      "+---------+-----------+--------------------+-----------------+--------------------+---+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM kafka_memory\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n",
      "| key|               value|topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|     0|2024-05-10 13:06:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|     1|2024-05-10 13:06:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|     2|2024-05-10 13:06:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|     3|2024-05-10 13:06:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|     4|2024-05-10 13:06:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|     5|2024-05-10 13:06:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|     6|2024-05-10 13:06:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|     7|2024-05-10 13:06:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|     8|2024-05-10 13:06:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|     9|2024-05-10 13:06:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|    10|2024-05-10 13:07:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|    11|2024-05-10 13:07:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|    12|2024-05-10 13:07:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|    13|2024-05-10 13:07:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|    14|2024-05-10 13:07:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|    15|2024-05-10 13:07:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|    16|2024-05-10 13:07:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|    17|2024-05-10 13:07:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|    18|2024-05-10 13:07:...|            0|\n",
      "|null|[7B 22 6E 61 6D 6...| user|        0|    19|2024-05-10 13:07:...|            0|\n",
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:19091,kafka2:19092,kafka3:19093\") \\\n",
    "    .option(\"subscribe\", \"user\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField(\"birthdate\", T.StringType()),\n",
    "    T.StructField(\"blood_group\", T.StringType()),\n",
    "    T.StructField(\"job\", T.StringType()),\n",
    "    T.StructField(\"name\", T.StringType()),\n",
    "    T.StructField(\"residence\", T.StringType()),\n",
    "    T.StructField(\"sex\", T.StringType()),\n",
    "    T.StructField(\"ssn\", T.StringType()),\n",
    "    T.StructField(\"uuid\", T.StringType()),\n",
    "    T.StructField(\"timestamp\", T.TimestampType()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "value_df = kafka_df.select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"value\"))\n",
    "\n",
    "processed_df = value_df.selectExpr(\n",
    "    \"value.birthdate\", \n",
    "    \"value.blood_group\", \n",
    "    \"value.job\",\n",
    "    \"value.name\",\n",
    "    \"value.residence\",\n",
    "    \"value.sex\",\n",
    "    \"value.ssn\",\n",
    "    \"value.uuid\",\n",
    "    \"value.timestamp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, KafkaAdminClient\n",
    "from kafka import TopicPartition, OffsetAndMetadata\n",
    "from kafka.admin import NewPartitions, NewTopic\n",
    "\n",
    "bootstrap_servers=[\"kafka1:19091\", \"kafka2:19092\", \"kafka3:19093\"]\n",
    "topic_name = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    topic_name,\n",
    "    bootstrap_servers = bootstrap_servers,\n",
    "    auto_offset_reset = \"earliest\",\n",
    ")\n",
    "consumer.partitions_for_topic(topic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'user', '__consumer_offsets']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)\n",
    "admin_client.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreatePartitionsResponse_v1(throttle_time_ms=0, topic_errors=[(topic='user', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# topic 개수 변경\n",
    "part = NewPartitions(6)\n",
    "m = {topic_name: part}\n",
    "admin_client.create_partitions(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='test', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# topic 생성\n",
    "new_topic = \"test\"\n",
    "\n",
    "new_topic_config = {\n",
    "    'name': new_topic,\n",
    "    'num_partitions': 3,\n",
    "    'replication_factor': 1  # Adjust as per your requirement\n",
    "}\n",
    "admin_client.create_topics([NewTopic(**new_topic_config)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subscribe to the topic\n",
    "consumer.subscribe(topics=[topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offset for topic 'test', partition 0: 0\n",
      "Offset for topic 'test', partition 1: 0\n",
      "Offset for topic 'test', partition 2: 0\n",
      "Total offset for topic 'test': 0\n"
     ]
    }
   ],
   "source": [
    "# Get the total offset\n",
    "total_offset = 0\n",
    "for partition in consumer.partitions_for_topic(topic_name):\n",
    "    tp = TopicPartition(topic_name, partition)\n",
    "    consumer.seek_to_end(tp)\n",
    "    print(f\"Offset for topic '{topic_name}', partition {partition}: {offset}\")\n",
    "    offset = consumer.position(tp)\n",
    "    total_offset += offset\n",
    "    \n",
    "print(f\"Total offset for topic '{topic_name}': {total_offset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
